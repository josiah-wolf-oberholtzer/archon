{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MlwrB7VvI4P"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtgDw2XGlsD9"
   },
   "outputs": [],
   "source": [
    "!pip install librosa==0.8.0\n",
    "import numpy as np\n",
    "import json as json\n",
    "import librosa\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPlalIPIp8P1"
   },
   "outputs": [],
   "source": [
    "destination_db = \"/content/drive/My Drive/Scammers_Sorted_PM\"  # @param {type:\"string\"}\n",
    "dest_datasize = \"10\"  # @param {type:\"string\"}\n",
    "slice_size = \"2\"  # @param {type:\"string\"}\n",
    "hop_length = \"2048\"  # @param {type:\"string\"}\n",
    "sr = \"44100\"  # @param {type:\"string\"}\n",
    "index_num = \"5\"  # @param {type:\"string\"}\n",
    "output_filename = (\n",
    "    \"/content/drive/My Drive/analysis_250ms.json\"  # @param {type:\"string\"}\n",
    ")\n",
    "analysis_filename = (\n",
    "    \"/content/drive/My Drive/analysis_desilenced.json\"  # @param {type:\"string\"}\n",
    ")\n",
    "\n",
    "hop = int(hop_length)\n",
    "sr = int(sr)\n",
    "slice_size = int(slice_size)\n",
    "index_num = int(index_num)\n",
    "dest_datasize = int(dest_datasize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxnR0SVj9D9a"
   },
   "outputs": [],
   "source": [
    "## STORE ANAYLSIS AS JSON\n",
    "\n",
    "\n",
    "def export_to_json(data, savefile=output_filename):\n",
    "\n",
    "    with open(savefile, \"a\") as outfile:\n",
    "        json.dump(data, outfile, indent=2)\n",
    "\n",
    "\n",
    "## IMPORT JSON FILE\n",
    "\n",
    "\n",
    "def json_load(filename):\n",
    "\n",
    "    f = open(filename)\n",
    "    l = json.load(f)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOAOeQCGzEE4"
   },
   "outputs": [],
   "source": [
    "## GRAB DESCRIPTORS FROM AUDIODB\n",
    "\n",
    "\n",
    "def grab_descriptors(filename, sr=sr):\n",
    "\n",
    "    y, sr = librosa.load(filename, sr=sr)\n",
    "\n",
    "    cent = np.median(\n",
    "        np.ndarray.flatten(\n",
    "            librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop)\n",
    "        )\n",
    "    )\n",
    "    flat = np.median(\n",
    "        np.ndarray.flatten(librosa.feature.spectral_flatness(y=y, hop_length=hop))\n",
    "    )\n",
    "    rolloff = np.median(\n",
    "        np.ndarray.flatten(librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop))\n",
    "    )\n",
    "    rms = np.median(np.ndarray.flatten(librosa.feature.rms(y=y, hop_length=hop)))\n",
    "\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "        y, fmin=librosa.note_to_hz(\"C2\"), fmax=librosa.note_to_hz(\"C7\")\n",
    "    )\n",
    "\n",
    "    voiced = np.median(np.ndarray.flatten(voiced_flag))\n",
    "\n",
    "    if voiced == True:\n",
    "        f0 = f0[~np.isnan(f0)]\n",
    "        pitch = np.median(np.ndarray.flatten(f0))\n",
    "        pitch = str(librosa.hz_to_note(pitch))\n",
    "        pitch = pitch.replace(\"â™¯\", \"#\")\n",
    "\n",
    "    else:\n",
    "        pitch = \"unpitched\"\n",
    "\n",
    "    dict_ = {\n",
    "        \"cent\": str(cent),\n",
    "        \"flat\": str(flat),\n",
    "        \"rolloff\": str(rolloff),\n",
    "        \"rms\": str(rms),\n",
    "        \"pitch\": pitch,\n",
    "    }\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def analyze_db(db=destination_db, samplerate=44100, outfile=output_filename):\n",
    "\n",
    "    dict_ = {}\n",
    "    print(\n",
    "        \"...looking for previous progress... WARN: this can take VERY LONG depending on directory size & layout!!\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_filename):\n",
    "        print(\"...loading previous progress...\")\n",
    "        dict_ = json_load(output_filename)\n",
    "        skiplen = len(dict_.keys())\n",
    "        print(\"OK: done, found \" + str(skiplen) + \" entries.\")\n",
    "    else:\n",
    "        print(\"WARN: no previous progress found.\")\n",
    "\n",
    "    counter = 0\n",
    "    skipcounter = 0\n",
    "\n",
    "    for element in os.scandir(db):\n",
    "        if element.is_dir():\n",
    "            for entry in os.scandir(element):\n",
    "                if entry.path.endswith(\".wav\"):\n",
    "                    if str(entry.name) in dict_.keys():\n",
    "                        skipcounter += 1\n",
    "                    else:\n",
    "                        try:\n",
    "                            dict_[str(entry.name)] = grab_descriptors(entry)\n",
    "                        except:\n",
    "                            print(\n",
    "                                \"ERR: file \"\n",
    "                                + str(entry.name)\n",
    "                                + \" could not be processed.\"\n",
    "                            )\n",
    "\n",
    "                counter += 1\n",
    "                if (counter % 5000) == 0:\n",
    "\n",
    "                    print(\"OK: \" + str(counter) + \" completed.\")\n",
    "                    if skipcounter > 0 & skipcounter < skiplen:\n",
    "                        print(\n",
    "                            \"OK: skipped \"\n",
    "                            + str(skipcounter)\n",
    "                            + \" files that have already been processed.\"\n",
    "                        )\n",
    "\n",
    "                    if skipcounter != counter:\n",
    "                        print(\"...saving... \")\n",
    "                        if os.path.exists(output_filename):\n",
    "                            os.remove(output_filename)\n",
    "                        export_to_json(dict_, output_filename)\n",
    "                        print(\"OK: saved!\")\n",
    "                    else:\n",
    "                        print(\"OK: still processing skipped files, no need to save.\")\n",
    "\n",
    "    if os.path.exists(output_filename):\n",
    "        os.remove(output_filename)\n",
    "    export_to_json(dict_, output_filename)\n",
    "    print(\"OK: successfully completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GurcuWcRrgxb"
   },
   "outputs": [],
   "source": [
    "## TO ANALYZE DB AND EXPORT TO JSON\n",
    "complete_analysis = analyze_db(destination_db)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def strip_silence(dict_):\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for k, v in dict_.copy().items():\n",
    "        sample = v\n",
    "        filename = k\n",
    "        if float(sample.get(\"rms\")) < 0.01:\n",
    "            dict_.pop(filename, None)\n",
    "            if os.path.exists(\n",
    "                destination_db + \"/\" + filename[:index_num] + \"/\" + filename\n",
    "            ):\n",
    "                os.remove(destination_db + \"/\" + filename[:index_num] + \"/\" + filename)\n",
    "            counter = counter + 1\n",
    "            print(\"removed \" + str(filename) + \", \" + str(counter) + \" items removed.\")\n",
    "\n",
    "    export_to_json(dict_, analysis_filename)"
   ],
   "metadata": {
    "id": "oDla7Bsht7Wp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "strip_silence(json_load(output_filename))\n",
    "print(\"OK: desilenced\")\n",
    "drive.flush_and_unmount()\n",
    "print(\"OK: successfully unmounted VM!\")"
   ],
   "metadata": {
    "id": "_Wdan87hvFih"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NVK-tEH-Wq4"
   },
   "outputs": [],
   "source": [
    "## FIRST PASS MAY TAKE A BIT AND/OR ERROR OUT IF DIR IS HUGE - JUST BE PATIENT AND RETRY, IT WILL GO THROUGH EVENTUALLY\n",
    "\n",
    "\n",
    "def sort_by_pitch(unsort_or_sort, dict_):\n",
    "\n",
    "    counter = 0\n",
    "    pitch = \"\"\n",
    "    print(\n",
    "        \"...loading directory... WARN: this can take VERY LONG depending on directory size & layout!!\"\n",
    "    )\n",
    "\n",
    "    for k, v in dict_.items():\n",
    "\n",
    "        sample = v\n",
    "        filename = k\n",
    "        counter += 1\n",
    "\n",
    "        pitch = sample.get(\"pitch\")\n",
    "\n",
    "        if pitch != \"unpitched\":\n",
    "            oct = int(pitch[-1])\n",
    "            pitch = pitch.replace(str(oct), \"\")\n",
    "            pitch_dir = destination_db + \"/\" + pitch + \"/\" + str(oct) + \"/\"\n",
    "\n",
    "        else:\n",
    "            pitch_dir = destination_db + \"/\" + pitch + \"/\"\n",
    "            cent = sample.get(\"cent\")\n",
    "            flat = sample.get(\"flat\")\n",
    "            rolloff = sample.get(\"rolloff\")\n",
    "\n",
    "            if float(cent) > 4000.0:\n",
    "                pitch_dir = pitch_dir + \"high_cent/\"\n",
    "            else:\n",
    "                pitch_dir = pitch_dir + \"low_cent/\"\n",
    "\n",
    "            if float(flat) > 0.01:\n",
    "                pitch_dir = pitch_dir + \"high_flat/\"\n",
    "            else:\n",
    "                pitch_dir = pitch_dir + \"low_flat/\"\n",
    "\n",
    "            if float(rolloff) > 8000:\n",
    "                pitch_dir = pitch_dir + \"high_rolloff/\"\n",
    "            else:\n",
    "                pitch_dir = pitch_dir + \"low_rollof/\"\n",
    "\n",
    "        if unsort_or_sort == \"sort\":\n",
    "            if os.path.exists(pitch_dir) == False:\n",
    "                os.makedirs(pitch_dir)\n",
    "            if os.path.exists(pitch_dir + filename) == False:\n",
    "                os.replace(\n",
    "                    destination_db + \"/\" + filename[:index_num] + \"/\" + filename,\n",
    "                    pitch_dir + filename,\n",
    "                )\n",
    "            else:\n",
    "                print(\"OK: File \" + pitch_dir + filename + \" has already been moved.\")\n",
    "\n",
    "        else:\n",
    "            if os.path.exists(pitch_dir + filename) == True:\n",
    "                os.rename(pitch_dir + filename, destination_db + filename)\n",
    "                print(destination_db + filename)\n",
    "\n",
    "        if counter == 1:\n",
    "            print(\"OK: beginning to sort.\")\n",
    "        if (counter % 5000) == 0:\n",
    "            print(\"OK: processed \" + str(counter) + \" files.\")\n",
    "\n",
    "    print(\n",
    "        \"OK: successfully completed! WARN: you need to flush and unmount the VM for changes to fully take effect!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dGxVnRs_nbW"
   },
   "outputs": [],
   "source": [
    "## TO MOVE FILES BY PITCH INTO SUBDIRECTORIES\n",
    "sort_by_pitch(\"sort\", json_load(analysis_filename))\n",
    "print(\"OK: sorted\")\n",
    "drive.flush_and_unmount()\n",
    "print(\"OK: successfully unmounted VM!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "archon_analyze.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.5 (clean)",
   "language": "python",
   "name": "clean3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}