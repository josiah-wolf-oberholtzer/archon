{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MlwrB7VvI4P",
        "outputId": "95fb44fa-3636-42a1-a9f9-e54cbe198b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtgDw2XGlsD9"
      },
      "outputs": [],
      "source": [
        "!pip install librosa==0.8.0\n",
        "import numpy as np\n",
        "import json as json\n",
        "import librosa\n",
        "import os, os.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPlalIPIp8P1"
      },
      "outputs": [],
      "source": [
        "destination_db = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/colab-violingan/archon-analysis/0-5s/\" #@param {type:\"string\"}\n",
        "dest_datasize = \"186704\" #@param {type:\"string\"}\n",
        "slice_size = \"2\" #@param {type:\"string\"}\n",
        "hop_length = \"2048\" #@param {type:\"string\"}\n",
        "sr = \"44100\" #@param {type:\"string\"}\n",
        "output_filename = \"/content/drive/My Drive/analysis_500ms.json\" #@param {type:\"string\"}\n",
        "analysis_filename = \"/content/drive/My Drive/analysis_f.json\" #@param {type:\"string\"}\n",
        "trial_filename = \"/content/drive/My Drive/trial.json\" #@param {type:\"string\"}\n",
        "\n",
        "hop = int(hop_length)\n",
        "sr = int(sr)\n",
        "slice_size = int(slice_size)\n",
        "dest_datasize = int(dest_datasize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo3wJzf7HWeK",
        "outputId": "b9d94baf-8f31-435f-bbbf-e6e27595e8e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186704\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL\n",
        "# dest_datasize = len(\n",
        "#  [name for name in os.listdir(destination_db) if os.path.isfile(\n",
        "#  os.path.join(destination_db, name))])\n",
        "# NOTE: depending on size of folder, this can error out multiple times - \n",
        "#Colab will cache the results, so retry until success and log the number for future attempts.\n",
        "print(dest_datasize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxnR0SVj9D9a"
      },
      "outputs": [],
      "source": [
        "## STORE ANAYLSIS AS JSON\n",
        "\n",
        "def export_to_json (data, savefile = output_filename):\n",
        "\n",
        "  with open(savefile, 'a') as outfile:\n",
        "    json.dump(data, outfile, indent=2)\n",
        "\n",
        "\n",
        "## IMPORT JSON FILE\n",
        "\n",
        "def json_load (filename):\n",
        "\n",
        "  f = open(filename)\n",
        "  l = json.load(f)\n",
        "  return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOAOeQCGzEE4"
      },
      "outputs": [],
      "source": [
        "## GRAB DESCRIPTORS FROM AUDIODB\n",
        "\n",
        "def grab_descriptors(filename, sr = sr):\n",
        "    \n",
        "  y, sr = librosa.load(filename, sr = sr)\n",
        "\n",
        "  cent = np.median(\n",
        "    np.ndarray.flatten(\n",
        "    librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop)))\n",
        "  flat = np.median(\n",
        "    np.ndarray.flatten(\n",
        "    librosa.feature.spectral_flatness(y=y, hop_length=hop)))\n",
        "  rolloff = np.median(\n",
        "    np.ndarray.flatten(\n",
        "    librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop)))\n",
        "  rms = np.median(\n",
        "    np.ndarray.flatten(\n",
        "    librosa.feature.rms(y=y, hop_length=hop)))\n",
        "    \n",
        "  f0, voiced_flag, voiced_probs = librosa.pyin(y,\n",
        "                                fmin=librosa.note_to_hz('C2'),\n",
        "                                fmax=librosa.note_to_hz('C7'))\n",
        "    \n",
        "  voiced = np.median(np.ndarray.flatten(voiced_flag))\n",
        "\n",
        "  if (voiced == True):\n",
        "    f0 = f0[~np.isnan(f0)]\n",
        "    pitch = np.median(np.ndarray.flatten(f0))\n",
        "    pitch = str(librosa.hz_to_note(pitch))\n",
        "    pitch = pitch.replace(\"â™¯\", \"#\") \n",
        "\n",
        "  else: \n",
        "    pitch = \"unpitched\"\n",
        "\n",
        "  dict_ = { \n",
        "        \"cent\": str(cent),\n",
        "        \"flat\": str(flat),\n",
        "        \"rolloff\": str(rolloff),\n",
        "        \"rms\": str(rms),\n",
        "        \"pitch\": pitch\n",
        "        }\n",
        "  \n",
        "  return dict_\n",
        "\n",
        "\n",
        "def analyze_db(db = destination_db, samplerate = 44100, outfile = output_filename):\n",
        "\n",
        "  dict_ = {}\n",
        "  print (\"...looking for previous progress... WARN: this can take VERY LONG depending on directory size & layout!!\")\n",
        "\n",
        "  if (os.path.exists(output_filename)): \n",
        "    print(\"...loading previous progress...\")\n",
        "    dict_ = json_load(output_filename)\n",
        "    skiplen = len(dict_.keys())\n",
        "    print(\"OK: done, found \" + str(skiplen) + \" entries.\")\n",
        "  else:\n",
        "    print (\"WARN: no previous progress found.\")\n",
        "    \n",
        "  counter = 0\n",
        "  skipcounter = 0\n",
        "\n",
        "  for entry in os.scandir(db):\n",
        "    if (entry.path.endswith(\".wav\")):\n",
        "      if (str(entry.name) in dict_.keys()): skipcounter += 1\n",
        "      else:\n",
        "        try: \n",
        "          dict_[str(entry.name)] = grab_descriptors(entry)\n",
        "        except: print (\"ERR: file \" + str(entry.name) + \" could not be processed.\")\n",
        "  \n",
        "    counter += 1\n",
        "    if ((counter % 5000) == 0): \n",
        "\n",
        "      print(\"OK: \" + str(counter) + \" completed out of \" + str(dest_datasize) + \".\")\n",
        "      if (skipcounter > 0 & skipcounter < skiplen): print(\"OK: skipped \" + str(skipcounter) + \" files that have already been processed.\")\n",
        "      \n",
        "      if (skipcounter != counter):\n",
        "        print(\"...saving... \")\n",
        "        if (os.path.exists(output_filename)): os.remove(output_filename)\n",
        "        export_to_json(dict_, output_filename)\n",
        "        print(\"OK: saved!\")\n",
        "      else: print(\"OK: still processing skipped files, no need to save.\")\n",
        "\n",
        "  if (os.path.exists(output_filename)): os.remove(output_filename)\n",
        "  export_to_json(dict_, output_filename)\n",
        "  print(\"OK: successfully completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GurcuWcRrgxb"
      },
      "outputs": [],
      "source": [
        "## TO ANALYZE DB AND EXPORT TO JSON\n",
        "complete_analysis = analyze_db(destination_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NVK-tEH-Wq4"
      },
      "outputs": [],
      "source": [
        "## FIRST PASS MAY TAKE A BIT AND/OR ERROR OUT IF DIR IS HUGE - JUST BE PATIENT AND RETRY, IT WILL GO THROUGH EVENTUALLY\n",
        "\n",
        "def sort_by_pitch (unsort_or_sort, dict_):\n",
        "\n",
        "  counter = 0 \n",
        "  pitch = \"\"\n",
        "  print (\"...loading directory... WARN: this can take VERY LONG depending on directory size & layout!!\")\n",
        "  \n",
        "  for k, v in dict_.items():\n",
        "\n",
        "    sample = v\n",
        "    filename = k\n",
        "    counter += 1\n",
        "\n",
        "    pitch = sample.get(\"pitch\")\n",
        "    \n",
        "    if (pitch != \"unpitched\"): \n",
        "      oct = int(pitch[-1])\n",
        "      pitch = pitch.replace(str(oct), \"\")  \n",
        "      pitch_dir = destination_db + pitch + \"/\" + str(oct) + \"/\"   \n",
        "\n",
        "    else: \n",
        "      pitch_dir = destination_db + pitch + \"/\"\n",
        "      cent = sample.get(\"cent\")\n",
        "      flat = sample.get(\"flat\")\n",
        "      rolloff = sample.get(\"rolloff\")\n",
        "\n",
        "      if float(cent) > 4000.0: pitch_dir = pitch_dir + \"high_cent/\"\n",
        "      else: pitch_dir = pitch_dir + \"low_cent/\"\n",
        "\n",
        "      if float(flat) > 0.01: pitch_dir = pitch_dir + \"high_flat/\"\n",
        "      else: pitch_dir = pitch_dir + \"low_flat/\"\n",
        "\n",
        "      if float(rolloff) > 8000: pitch_dir = pitch_dir + \"high_rolloff/\"\n",
        "      else: pitch_dir = pitch_dir + \"low_rollof/\"     \n",
        "\n",
        "    if (unsort_or_sort == \"sort\"):     \n",
        "      if (os.path.exists(pitch_dir) == False): os.makedirs(pitch_dir)\n",
        "      if (os.path.exists(pitch_dir + filename) == False): os.replace(destination_db + filename, pitch_dir + filename)\n",
        "      else: print (\"OK: File \" + pitch_dir + filename + \" has already been moved.\")\n",
        "\n",
        "    else: \n",
        "      if (os.path.exists(pitch_dir + filename) == True): \n",
        "        os.rename(pitch_dir + filename, destination_db + filename)\n",
        "        print(destination_db + filename)\n",
        "    \n",
        "    if (counter == 1): print(\"OK: beginning to sort.\")\n",
        "    if ((counter % 5000) == 0): print(\"OK: processed \" + str(counter) + \" files.\")\n",
        "  \n",
        "  print(\"OK: successfully completed! WARN: you need to flush and unmount the VM for changes to fully take effect!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGxVnRs_nbW"
      },
      "outputs": [],
      "source": [
        "## TO MOVE FILES BY PITCH INTO SUBDIRECTORIES\n",
        "sort_by_pitch (\"sort\", json_load(output_filename))\n",
        "print(\"OK: sorted\")\n",
        "drive.flush_and_unmount()\n",
        "print(\"OK: successfully unmounted VM!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "archon_analyze.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.5 (clean)",
      "language": "python",
      "name": "clean3.5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}